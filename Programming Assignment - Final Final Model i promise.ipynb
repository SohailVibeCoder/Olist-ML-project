{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLdVwXByiLGhwiPlXqdOqh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohailVibeCoder/Olist-ML-project/blob/main/Programming%20Assignment%20-%20Final%20Final%20Model%20i%20promise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning & Feature Engineering"
      ],
      "metadata": {
        "id": "IaNBVhfiyW_v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in73CRBVzDQC",
        "outputId": "7d940ae3-495d-4506-d0a3-e391beedb01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# --- 1. SETUP and INITIAL DATA LOADING ---\n",
        "drive.mount('/content/drive')\n",
        "# Assuming your Olist files are in your Drive root folder\n",
        "os.chdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "# Load all required original datasets\n",
        "orders_df = pd.read_csv(\"olist_orders_dataset.csv\")\n",
        "order_reviews_df = pd.read_csv(\"olist_order_reviews_dataset.csv\")\n",
        "order_payments_df = pd.read_csv(\"olist_order_payments_dataset.csv\")\n",
        "customers_df = pd.read_csv(\"olist_customers_dataset.csv\")\n",
        "order_items_df = pd.read_csv(\"olist_order_items_dataset.csv\")\n",
        "products_df = pd.read_csv(\"olist_products_dataset.csv\")\n",
        "sellers_df = pd.read_csv(\"olist_sellers_dataset.csv\")\n",
        "translation_df = pd.read_csv(\"product_category_name_translation.csv\")\n",
        "geolocation_df = pd.read_csv(\"olist_geolocation_dataset.csv\") # <-- Geo Data\n",
        "\n",
        "\n",
        "# --- 2. PERFORM INITIAL MERGE and CLEANING ---\n",
        "full_df = (\n",
        "    orders_df\n",
        "    .merge(order_reviews_df, on='order_id', how='left')\n",
        "    .merge(order_payments_df, on='order_id', how='left')\n",
        "    .merge(customers_df, on='customer_id', how='left')\n",
        "    .merge(order_items_df, on='order_id', how='left')\n",
        "    .merge(products_df, on='product_id', how='left')\n",
        "    .merge(sellers_df, on='seller_id', how='left')\n",
        "    .merge(translation_df, on='product_category_name', how='left')\n",
        ")\n",
        "\n",
        "full_df.to_csv(\"RawMergedData.csv\", index=False)\n",
        "\n",
        "\n",
        "# Filter Delivered, Convert Dates, Drop Core Nulls\n",
        "full_df = full_df[full_df['order_status'] == 'delivered'].copy()\n",
        "date_cols = [\n",
        "    'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
        "    'order_delivered_customer_date', 'order_estimated_delivery_date', 'shipping_limit_date',\n",
        "    'review_creation_date', 'review_answer_timestamp'\n",
        "]\n",
        "for col in date_cols:\n",
        "    full_df[col] = pd.to_datetime(full_df[col], errors='coerce')\n",
        "full_df.dropna(subset=['price', 'product_id', 'review_score', 'product_category_name_english'], inplace=True)\n",
        "with pd.option_context('mode.chained_assignment', None):\n",
        "    full_df['review_comment_title'].fillna('', inplace=True)\n",
        "    full_df['review_comment_message'].fillna('', inplace=True)\n",
        "\n",
        "# --- 3. GEOLOCATION AGGREGATION AND MERGE ---\n",
        "# Get median lat/lng for each zip code prefix\n",
        "geo_median = geolocation_df.groupby('geolocation_zip_code_prefix').agg(\n",
        "    geolocation_lat=('geolocation_lat', 'median'),\n",
        "    geolocation_lng=('geolocation_lng', 'median')\n",
        ").reset_index()\n",
        "\n",
        "# Merge customer lat/lng\n",
        "geo_customer = geo_median.rename(columns={'geolocation_zip_code_prefix': 'customer_zip_code_prefix', 'geolocation_lat': 'customer_lat', 'geolocation_lng': 'customer_lng'})\n",
        "full_df = full_df.merge(geo_customer, on='customer_zip_code_prefix', how='left')\n",
        "\n",
        "# Merge seller lat/lng\n",
        "geo_seller = geo_median.rename(columns={'geolocation_zip_code_prefix': 'seller_zip_code_prefix', 'geolocation_lat': 'seller_lat', 'geolocation_lng': 'seller_lng'})\n",
        "full_df = full_df.merge(geo_seller, on='seller_zip_code_prefix', how='left')\n",
        "\n",
        "# Drop rows missing necessary geo-coordinates\n",
        "full_df.dropna(subset=['customer_lat', 'customer_lng', 'seller_lat', 'seller_lng'], inplace=True)\n",
        "df = full_df.copy()\n",
        "\n",
        "# =================================================================\n",
        "# ADVANCED FEATURE ENGINEERING (New Geo + Old RFM/Product Logic)\n",
        "# =================================================================\n",
        "\n",
        "# --- 4. Haversine Distance & K-Means Clustering ---\n",
        "\n",
        "# Haversine Distance Function\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return 6371 * c\n",
        "\n",
        "df['distance_km'] = haversine(\n",
        "    df['customer_lat'], df['customer_lng'],\n",
        "    df['seller_lat'], df['seller_lng']\n",
        ")\n",
        "\n",
        "# K-Means Clustering (K=10)\n",
        "geo_coords = df[['customer_lat', 'customer_lng']].values\n",
        "scaler = StandardScaler()\n",
        "geo_coords_scaled = scaler.fit_transform(geo_coords)\n",
        "K = 10\n",
        "kmeans = KMeans(n_clusters=K, random_state=42, n_init='auto', max_iter=300, verbose=0)\n",
        "df['customer_geo_cluster'] = kmeans.fit_predict(geo_coords_scaled)\n",
        "df['customer_geo_cluster'] = df['customer_geo_cluster'].astype(object)\n",
        "\n",
        "\n",
        "# --- 5. Create RFM, Delivery, and Volume Features (as in your original code) ---\n",
        "\n",
        "# A. RFM Features\n",
        "snapshot_date = df['order_purchase_timestamp'].max() + pd.Timedelta(days=1)\n",
        "rfm_df = df.groupby('customer_unique_id').agg(\n",
        "    Recency=('order_purchase_timestamp', lambda x: (snapshot_date - x.max()).days),\n",
        "    Frequency=('order_id', 'nunique'),\n",
        "    Monetary=('payment_value', 'sum')\n",
        ").reset_index()\n",
        "df = df.merge(rfm_df, on='customer_unique_id', how='left')\n",
        "\n",
        "# B. Delivery Metrics\n",
        "df['delivery_time_delta'] = df['order_delivered_customer_date'] - df['order_purchase_timestamp']\n",
        "df['actual_delivery_days'] = df['delivery_time_delta'].dt.days.fillna(df['delivery_time_delta'].dt.days.median())\n",
        "df['delivery_performance_days'] = (\n",
        "    df['order_estimated_delivery_date'] - df['order_delivered_customer_date']\n",
        ").dt.days.fillna(0)\n",
        "\n",
        "# C. Product Volume\n",
        "df['product_volume_cm3'] = (df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm'])\n",
        "volume_median = df['product_volume_cm3'].median()\n",
        "with pd.option_context('mode.chained_assignment', None):\n",
        "    df['product_volume_cm3'].fillna(volume_median, inplace=True)\n",
        "\n",
        "\n",
        "# --- 6. One-Hot Encoding and Product Category Aggregation ---\n",
        "\n",
        "# Categorical columns now include the new customer_geo_cluster\n",
        "categorical_cols = ['product_category_name_english', 'payment_type', 'customer_geo_cluster']\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols, drop_first=True)\n",
        "\n",
        "# Product Category Aggregation (as in your original code)\n",
        "columns_to_drop_after_agg = []\n",
        "product_prefix = 'product_category_name_english_'\n",
        "category_map = {\n",
        "    'Home_Kitchen_Comfort': ['air_conditioning', 'home_appliances', 'home_appliances_2', 'home_comfort_2', 'home_confort', 'housewares', 'kitchen_dining_laundry_garden_furniture', 'la_cuisine', 'small_appliances', 'small_appliances_home_oven_and_coffee'],\n",
        "    'Fashion_Apparel': ['fashio_female_clothing', 'fashion_male_clothing', 'fashion_childrens_clothes', 'fashion_underwear_beach', 'fashion_shoes', 'fashion_sport', 'fashion_bags_accessories', 'luggage_accessories', 'watches_gifts'],\n",
        "    'Electronics_Technology': ['audio', 'cine_photo', 'computers', 'computers_accessories', 'consoles_games', 'electronics', 'fixed_telephony', 'music', 'musical_instruments', 'tablets_printing_image', 'telephony'],\n",
        "    'Tools_Construction': ['construction_tools_construction', 'construction_tools_lights', 'construction_tools_safety', 'costruction_tools_garden', 'costruction_tools_tools', 'garden_tools', 'home_construction'],\n",
        "    'Furniture_Decor': ['bed_bath_table', 'furniture_bedroom', 'furniture_decor', 'furniture_living_room', 'furniture_mattress_and_upholstery', 'office_furniture'],\n",
        "    'Books_Media_Toys': ['books_general_interest', 'books_imported', 'books_technical', 'cds_dvds_musicals', 'dvds_blu_ray', 'toys', 'cool_stuff'],\n",
        "    'Health_Personal_Care': ['health_beauty', 'perfumery', 'baby', 'diapers_and_hygiene'],\n",
        "    'Art_Hobbies': ['art', 'arts_and_craftmanship', 'stationery', 'party_supplies', 'christmas_supplies'],\n",
        "    'Food_Drink': ['drinks', 'food', 'food_drink'],\n",
        "    'Automotive': ['auto'],\n",
        "    'Other_Services': ['flowers', 'industry_commerce_and_business', 'market_place', 'pet_shop', 'security_and_services', 'signaling_and_security']\n",
        "}\n",
        "for new_category, detailed_categories in category_map.items():\n",
        "    original_cols = [product_prefix + cat for cat in detailed_categories]\n",
        "    existing_cols = [col for col in original_cols if col in df_encoded.columns]\n",
        "    if existing_cols:\n",
        "        df_encoded[new_category] = df_encoded[existing_cols].any(axis=1).astype(int)\n",
        "        columns_to_drop_after_agg.extend(existing_cols)\n",
        "sports_leisure_col = product_prefix + 'sports_leisure'\n",
        "if sports_leisure_col in df_encoded.columns:\n",
        "    df_encoded.rename(columns={sports_leisure_col: 'Sports_Leisure'}, inplace=True)\n",
        "df_encoded = df_encoded.drop(columns=columns_to_drop_after_agg, errors='ignore')\n",
        "\n",
        "\n",
        "# --- 7. Final Drop of Irrelevant or Redundant Columns ---\n",
        "# IMPORTANT: This list ensures all raw IDs and the old state/region data are dropped.\n",
        "columns_to_drop_final = [\n",
        "    'order_id', 'customer_id', 'review_id', 'product_id', 'seller_id', 'customer_unique_id',\n",
        "    'order_status', 'review_comment_title', 'review_comment_message', 'customer_city', 'seller_city',\n",
        "    'product_category_name', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
        "    'order_delivered_customer_date', 'order_estimated_delivery_date', 'review_creation_date',\n",
        "    'review_answer_timestamp', 'shipping_limit_date', 'product_length_cm', 'product_height_cm',\n",
        "    'product_width_cm', 'customer_zip_code_prefix', 'seller_zip_code_prefix', 'delivery_time_delta',\n",
        "    'order_item_id', 'product_category_name_english', 'payment_sequential', # <-- Ensure payment_sequential is dropped\n",
        "    # DROPPING RAW GEO COORDINATES (now replaced by distance_km and clusters)\n",
        "    'customer_lat', 'customer_lng', 'seller_lat', 'seller_lng',\n",
        "    # DROPPING the old state columns that were used to create regions (Replaced by K-Means Clusters)\n",
        "    'customer_state', 'seller_state'\n",
        "]\n",
        "\n",
        "final_ml_df = df_encoded.drop(columns=columns_to_drop_final, errors='ignore')\n",
        "final_ml_df.to_csv(\"ml_ready_feature_table_V2.csv\", index=False)\n",
        "print(f\"\\n✅ Feature engineering complete. Final dataset saved as 'ml_ready_feature_table_V2.csv'. Shape: {final_ml_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Classifier"
      ],
      "metadata": {
        "id": "dNONNYuMyRnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 1: DATA LOADING AND SETUP\n",
        "# =================================================================\n",
        "# Load the final, advanced feature dataset\n",
        "df_ml = pd.read_csv(\"ml_ready_feature_table_V2.csv\")\n",
        "print(\"dataset loaded. Shape:\", df_ml.shape)\n",
        "\n",
        "# Target variable: 1 = Good (≥4), 0 = Bad (<4)\n",
        "y_class = (df_ml['review_score'] >= 4).astype(int)\n",
        "\n",
        "# Drop review_score (target)\n",
        "X = df_ml.drop(columns=['review_score'])\n",
        "\n",
        "# --- 2. Train-Test Split and Class Weighting ---\n",
        "X_train, X_test, y_train_class, y_test_class = train_test_split(\n",
        "    X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "# Prepare evaluation set\n",
        "eval_set = [\n",
        "    (X_train, y_train_class),\n",
        "    (X_test, y_test_class)\n",
        "]\n",
        "\n",
        "# Calculate imbalance ratio for scale_pos_weight\n",
        "count_class_1 = y_train_class.value_counts()[1]\n",
        "count_class_0 = y_train_class.value_counts()[0]\n",
        "scale_pos_weight = count_class_1 / count_class_0\n",
        "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 2: MODEL TRAINING\n",
        "# =================================================================\n",
        "# --- 3. Train XGBoost Classifier (Using the standard hyper-parameters) ---\n",
        "xgb_final_model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    eval_metric=\"logloss\"\n",
        ")\n",
        "print(\"\\nTraining XGBoost Model...\")\n",
        "xgb_final_model.fit(\n",
        "    X_train,\n",
        "    y_train_class,\n",
        "    eval_set=eval_set,\n",
        "    verbose=False\n",
        ")\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 3: EVALUATION AND FEATURE IMPORTANCE\n",
        "# =================================================================\n",
        "# --- 5. Threshold Optimization (Focusing on the minority class: Bad Review) ---\n",
        "y_proba = xgb_final_model.predict_proba(X_test)\n",
        "y_proba_minority = y_proba[:, 0]  # Probability of being class 0 (bad review)\n",
        "\n",
        "# Find the optimal threshold for the best F1 score on the Bad Review class\n",
        "thresholds = np.linspace(0.50, 0.10, 20)\n",
        "best_f1, best_threshold = 0, 0\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_temp = np.where(y_proba_minority > threshold, 0, 1)\n",
        "    # Calculate F1 for the minority class (pos_label=0)\n",
        "    f1 = f1_score(y_test_class, y_pred_temp, pos_label=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_threshold = f1, threshold\n",
        "\n",
        "print(f\"\\nOptimal threshold: {best_threshold:.3f} (F1 for bad class = {best_f1:.3f})\")\n",
        "\n",
        "\n",
        "# --- 6. Final Evaluation (TEST Set + Confusion Matrix) ---\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred_final = np.where(y_proba_minority > best_threshold, 0, 1)\n",
        "\n",
        "print(\"\\n---  MODEL TEST SET PERFORMANCE ---\")\n",
        "print(classification_report(y_test_class, y_pred_final, target_names=['0 (Bad)', '1 (Good)']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_class, y_pred_final)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test Set):\")\n",
        "print(cm)\n",
        "\n",
        "# Pretty-print version\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=['Actual Bad (0)', 'Actual Good (1)'],\n",
        "    columns=['Predicted Bad (0)', 'Predicted Good (1)']\n",
        ")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Formatted):\")\n",
        "print(cm_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "C5qGqTr8zORK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa69efb9-69ac-4312-bd89-bcfad87ef128"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset loaded. Shape: (112657, 40)\n",
            "Calculated scale_pos_weight: 3.31\n",
            "\n",
            "Training XGBoost Model...\n",
            "Model training complete.\n",
            "\n",
            "Optimal threshold: 0.100 (F1 for bad class = 0.517)\n",
            "\n",
            "---  MODEL TEST SET PERFORMANCE ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     0 (Bad)       0.50      0.54      0.52      5223\n",
            "    1 (Good)       0.86      0.84      0.85     17309\n",
            "\n",
            "    accuracy                           0.77     22532\n",
            "   macro avg       0.68      0.69      0.68     22532\n",
            "weighted avg       0.77      0.77      0.77     22532\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 2799  2424]\n",
            " [ 2801 14508]]\n",
            "\n",
            "Confusion Matrix (Formatted):\n",
            "                 Predicted Bad (0)  Predicted Good (1)\n",
            "Actual Bad (0)                2799                2424\n",
            "Actual Good (1)               2801               14508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearsons Coefficient Barchart (Magnitude adjusted)\n",
        "\n"
      ],
      "metadata": {
        "id": "1RUyuL4JxiAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# --- 1. Load the advanced ML-ready dataset ---\n",
        "df_original = pd.read_csv(\"ml_ready_feature_table_GEO_V2.csv\")\n",
        "\n",
        "# --- 2. Calculate Model-Agnostic Feature Importance (Absolute Correlation) ---\n",
        "# Create the binary target variable (1 for good review, 0 for bad review)\n",
        "df_original['review_score_binary'] = (df_original['review_score'] >= 4).astype(int)\n",
        "TARGET_COLUMN = 'review_score_binary'\n",
        "\n",
        "# Drop only the original review score column, as all other irrelevant columns\n",
        "# (like payment_sequential) were already dropped in the Feature Engineering phase.\n",
        "columns_to_exclude = ['review_score']\n",
        "df_corr = df_original.drop(columns=columns_to_exclude, errors='ignore')\n",
        "\n",
        "# Clean and ensure all columns are numeric\n",
        "for col in df_corr.columns:\n",
        "    if df_corr[col].dtype == 'bool':\n",
        "        df_corr[col] = df_corr[col].astype(int)\n",
        "df_corr.fillna(0, inplace=True)\n",
        "\n",
        "# Calculate absolute correlation with the binary target\n",
        "correlation_matrix = df_corr.corr()\n",
        "target_correlation = correlation_matrix[TARGET_COLUMN].drop(TARGET_COLUMN)\n",
        "absolute_correlation = target_correlation.abs()\n",
        "\n",
        "# Select the top 10 most correlated features\n",
        "top_features_corr = absolute_correlation.sort_values(ascending=False).head(10)\n",
        "\n",
        "print(\"\\nTop 10 Features based on Absolute Correlation with Binary Review Score (Geo V2):\")\n",
        "print(top_features_corr)\n",
        "\n",
        "\n",
        "# --- 3. Generate the Bar Chart ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_features_corr.sort_values(ascending=True).plot(kind='barh', color='darkgreen')\n",
        "plt.title(\n",
        "    'Top 10 Feature Strengths: Absolute Pearson Correlation (Advanced Geo V2)',\n",
        "    fontsize=14\n",
        ")\n",
        "plt.xlabel(\"Absolute Correlation Coefficient ($|r|$)\", fontsize=12)\n",
        "plt.ylabel(\"Feature Name\", fontsize=12)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_features_correlation_geo_v2.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nBar chart saved as 'top_features_correlation_geo_v2.png'.\")"
      ],
      "metadata": {
        "id": "l0_z-q3ZOmxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d08c5e5-715f-465e-df29-0016f9dd59a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Features based on Absolute Correlation with Binary Review Score (Geo V2):\n",
            "actual_delivery_days         0.268820\n",
            "delivery_performance_days    0.200433\n",
            "Monetary                     0.083553\n",
            "payment_value                0.067759\n",
            "Furniture_Decor              0.065275\n",
            "customer_geo_cluster_1       0.047985\n",
            "distance_km                  0.047473\n",
            "payment_installments         0.042257\n",
            "freight_value                0.028926\n",
            "Books_Media_Toys             0.028753\n",
            "Name: review_score_binary, dtype: float64\n",
            "\n",
            "Bar chart saved as 'top_features_correlation_geo_v2.png'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Load the dataset ---\n",
        "df_original = pd.read_csv(\"ml_ready_feature_table_GEO_V2.csv\")\n",
        "\n",
        "# --- 2. Create binary satisfaction target ---\n",
        "df_original['review_score_binary'] = (df_original['review_score'] >= 4).astype(int)\n",
        "TARGET_COLUMN = 'review_score_binary'\n",
        "\n",
        "# Remove raw score\n",
        "df_corr = df_original.drop(columns=['review_score'], errors='ignore')\n",
        "\n",
        "# Clean boolean columns\n",
        "for col in df_corr.columns:\n",
        "    if df_corr[col].dtype == 'bool':\n",
        "        df_corr[col] = df_corr[col].astype(int)\n",
        "\n",
        "df_corr.fillna(0, inplace=True)\n",
        "\n",
        "# --- 3. Pearson correlations ---\n",
        "corr_matrix = df_corr.corr()\n",
        "target_corr = corr_matrix[TARGET_COLUMN].drop(TARGET_COLUMN)\n",
        "\n",
        "# --- 4. Select only negative features (dissatisfaction drivers) ---\n",
        "negative_corr = target_corr[target_corr < 0]\n",
        "\n",
        "# Top 5 most negative correlations\n",
        "top5_negative = negative_corr.sort_values().head(5)\n",
        "\n",
        "# --- 5. Business-friendly naming ---\n",
        "#name_map = {\n",
        "   # 'delivery_performance_days': 'Delivery: Days Ahead/Behind Estimate',\n",
        "  #  'actual_delivery_days': 'Delivery: Total Shipping Days',\n",
        "   # 'distance_km': 'Logistics: Customer-Seller Distance (KM)',\n",
        "   # 'Monetary': 'Customer Value (Monetary)',\n",
        "   # 'payment_value': 'Total Payment Value',\n",
        "   # 'Furniture_Decor': 'Product: Furniture/Decor',\n",
        "   # 'customer_geo_cluster_1': 'Geo Hub 1',\n",
        "   # 'payment_installments': 'Payment Installments',\n",
        "   # 'freight_value': 'Item Freight Value',\n",
        "   # 'Books_Media_Toys': 'Product: Books/Media/Toys'\n",
        "#}\n",
        "\n",
        "#top5_negative.index = top5_negative.index.map(lambda x: name_map.get(x, x))\n",
        "\n",
        "# Convert to positive for plotting (but it's still negative behind the scenes)\n",
        "top5_plot = top5_negative.abs().sort_values(ascending=False)\n",
        "\n",
        "# --- 6. Plot: clean, blue gradient, no outlines, no numbers ---\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Blue gradient\n",
        "colors = sns.color_palette(\"Blues\", n_colors=len(top5_plot))\n",
        "\n",
        "plt.bar(\n",
        "    x=np.arange(len(top5_plot)),\n",
        "    height=top5_plot.values,\n",
        "    color=colors\n",
        ")\n",
        "\n",
        "plt.xticks(\n",
        "    ticks=np.arange(len(top5_plot)),\n",
        "    labels=top5_plot.index,\n",
        "    rotation=45,\n",
        "    ha='right',\n",
        "    fontsize=10\n",
        ")\n",
        "\n",
        "plt.title(\n",
        "    \"Top 5 Drivers of Customer Dissatisfaction\",\n",
        "    fontsize=16,\n",
        "    fontweight=\"bold\",\n",
        "    pad=15\n",
        ")\n",
        "\n",
        "plt.ylabel(\"Pearsons Correlation Coefficient\", fontsize=12)\n",
        "plt.xlabel(\"Feature\", fontsize=12)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
        "sns.despine(left=True, bottom=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"top_5_negative_pearson_.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nSaved as 'top_5_negative_pearson.png'\")\n"
      ],
      "metadata": {
        "id": "5884JCOdTqFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36af98bb-52d5-4ea5-bd1f-755616165f3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved as 'top_5_negative_pearson.png'\n"
          ]
        }
      ]
    }
  ]
}