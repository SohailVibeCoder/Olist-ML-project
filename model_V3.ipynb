{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohailVibeCoder/Olist-ML-project/blob/main/model_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning, aggregation and feature engineering"
      ],
      "metadata": {
        "id": "d3ty60h1PLQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUZE7LqekR78",
        "outputId": "61afd865-de31-4609-e216-376235985ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Initial raw DataFrame row count (No Duplication): 99992\n",
            "\n",
            "✅ Feature engineering complete. Final corrected dataset saved as 'ml_ready_feature_table_V3.csv'. Shape: (95879, 28)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ================================================================\n",
        "# 1. DATA LOADING — Mount Drive and Import All Olist Raw Tables\n",
        "#    (No transformation here, just getting all source data in.)\n",
        "# ================================================================\n",
        "drive.mount('/content/drive')\n",
        "# Assuming your Olist files are in your Drive root folder\n",
        "os.chdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "# Load all required original datasets (using variables defined by the user previously)\n",
        "orders_df = pd.read_csv(\"olist_orders_dataset.csv\")\n",
        "order_reviews_df = pd.read_csv(\"olist_order_reviews_dataset.csv\")\n",
        "order_payments_df = pd.read_csv(\"olist_order_payments_dataset.csv\")\n",
        "customers_df = pd.read_csv(\"olist_customers_dataset.csv\")\n",
        "order_items_df = pd.read_csv(\"olist_order_items_dataset.csv\")\n",
        "products_df = pd.read_csv(\"olist_products_dataset.csv\")\n",
        "sellers_df = pd.read_csv(\"olist_sellers_dataset.csv\")\n",
        "translation_df = pd.read_csv(\"product_category_name_translation.csv\")\n",
        "geolocation_df = pd.read_csv(\"olist_geolocation_dataset.csv\")\n",
        "\n",
        "# =================================================================\n",
        "# 2. ORDER-LEVEL AGGREGATION TO PREVENT DUPLICATION\n",
        "# -----------------------------------------------------------------\n",
        "# The raw order_items table has one row per (order, product).\n",
        "# For modelling we want ONE ROW PER ORDER, so we:\n",
        "#   - enrich items with product, translation and seller metadata,\n",
        "#   - aggregate item-level metrics (counts, sums, medians, modes)\n",
        "#     back to order_id level,\n",
        "#   - aggregate payments to order_id level as well.\n",
        "# This gives us clean order-level features like total_items,\n",
        "# total_product_price, seller_count, etc., without duplicate labels.\n",
        "# =================================================================\n",
        "\n",
        "# 1. Prepare Item Data by joining with Product, Translation, AND SELLERS tables\n",
        "items_enriched = order_items_df.merge(products_df, on='product_id', how='left')\n",
        "items_enriched = items_enriched.merge(translation_df, on='product_category_name', how='left')\n",
        "# Merge sellers_df to get seller_zip_code_prefix and other seller attributes\n",
        "items_enriched = items_enriched.merge(sellers_df, on='seller_id', how='left')\n",
        "\n",
        "# Ensure shipping_limit_date is datetime before aggregation\n",
        "items_enriched['shipping_limit_date'] = pd.to_datetime(items_enriched['shipping_limit_date'], errors='coerce')\n",
        "\n",
        "# 2. Aggregate the item details back to a single row per Order ID\n",
        "items_agg = items_enriched.groupby('order_id').agg(\n",
        "    total_items=('order_item_id', 'count'),\n",
        "    total_freight=('freight_value', 'sum'),\n",
        "    total_product_price=('price', 'sum'),\n",
        "    seller_count=('seller_id', 'nunique'),\n",
        "    main_product_category=('product_category_name_english', lambda x: x.mode()[0] if not x.mode().empty else 'unknown'),\n",
        "    # Use median for product physical features\n",
        "    product_name_lenght=('product_name_lenght', 'median'),\n",
        "    product_description_lenght=('product_description_lenght', 'median'),\n",
        "    product_photos_qty=('product_photos_qty', 'median'),\n",
        "    product_weight_g=('product_weight_g', 'median'),\n",
        "    product_length_cm=('product_length_cm', 'median'),\n",
        "    product_height_cm=('product_height_cm', 'median'),\n",
        "    product_width_cm=('product_width_cm', 'median'),\n",
        "    # Add shipping_limit_date\n",
        "    shipping_limit_date=('shipping_limit_date', 'max'),\n",
        "    # This column now exists due to the merge above:\n",
        "    seller_zip_code_prefix=('seller_zip_code_prefix', lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n",
        ").reset_index()\n",
        "\n",
        "# Aggregate payments to order-level financial features\n",
        "pay_agg = order_payments_df.groupby('order_id').agg(\n",
        "    payment_sequential=('payment_sequential', 'count'), # Count of payment types used\n",
        "    payment_installments=('payment_installments', 'max'), # Max installments used\n",
        "    payment_value=('payment_value', 'sum') # Total payment value\n",
        ").reset_index()\n",
        "\n",
        "# --- 4. PERFORM NON-DUPLICATING MERGE and CLEANING ---\n",
        "\n",
        "full_df = (\n",
        "    orders_df\n",
        "    .merge(order_reviews_df, on='order_id', how='left')\n",
        "    .merge(pay_agg, on='order_id', how='left') # merge aggregated payments\n",
        "    .merge(customers_df, on='customer_id', how='left')\n",
        "    .merge(items_agg, on='order_id', how='left')\n",
        ")\n",
        "\n",
        "print(f\"Initial raw DataFrame row count (No Duplication): {len(full_df)}\")\n",
        "\n",
        "# Filter Delivered, Convert Dates, Drop Core Nulls\n",
        "full_df = full_df[full_df['order_status'] == 'delivered'].copy()\n",
        "date_cols = [\n",
        "    'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
        "    'order_delivered_customer_date', 'order_estimated_delivery_date', 'review_creation_date',\n",
        "    'review_answer_timestamp' # Note: 'shipping_limit_date' is already a datetime from the aggregation step above\n",
        "]\n",
        "for col in date_cols:\n",
        "    full_df[col] = pd.to_datetime(full_df[col], errors='coerce')\n",
        "\n",
        "# Now we work with aggregated features instead of item-level ones\n",
        "full_df.dropna(subset=['total_product_price', 'total_freight', 'review_score', 'main_product_category'], inplace=True)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    full_df['review_comment_title'].fillna('', inplace=True)\n",
        "    full_df['review_comment_message'].fillna('', inplace=True)\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# 4. GEOLOCATION AGGREGATION AND MERGE\n",
        "# -----------------------------------------------------------------\n",
        "# The geolocation table is at ZIP prefix level. Here we:\n",
        "#   - compute median latitude/longitude for each ZIP prefix,\n",
        "#   - map those coords onto customers and sellers via their ZIP prefixes,\n",
        "#   - drop orders where geo info is missing.\n",
        "# This creates numeric spatial features that we will later turn into\n",
        "# distance_km and geo-clusters.\n",
        "# =================================================================\n",
        "\n",
        "# Get median lat/lng for each zip code prefix\n",
        "geo_median = geolocation_df.groupby('geolocation_zip_code_prefix').agg(\n",
        "    geolocation_lat=('geolocation_lat', 'median'),\n",
        "    geolocation_lng=('geolocation_lng', 'median')\n",
        ").reset_index()\n",
        "\n",
        "df = full_df.copy()\n",
        "\n",
        "# Merge customer lat/lng\n",
        "geo_customer = geo_median.rename(columns={'geolocation_zip_code_prefix': 'customer_zip_code_prefix', 'geolocation_lat': 'customer_lat', 'geolocation_lng': 'customer_lng'})\n",
        "df = df.merge(geo_customer, on='customer_zip_code_prefix', how='left')\n",
        "\n",
        "# Merge seller lat/lng\n",
        "geo_seller = geo_median.rename(columns={'geolocation_zip_code_prefix': 'seller_zip_code_prefix', 'geolocation_lat': 'seller_lat', 'geolocation_lng': 'seller_lng'})\n",
        "df = df.merge(geo_seller, on='seller_zip_code_prefix', how='left')\n",
        "\n",
        "# Drop rows missing necessary geo-coordinates (necessary for downstream features)\n",
        "df.dropna(subset=['customer_lat', 'customer_lng', 'seller_lat', 'seller_lng'], inplace=True)\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# 5. ADVANCED FEATURE ENGINEERING (Geo + RFM + Delivery + Volume)\n",
        "# -----------------------------------------------------------------\n",
        "# This section builds the main predictive signals:\n",
        "#   - distance_km: great-circle distance between seller and customer\n",
        "#   - customer_geo_cluster: K-Means cluster ID based on customer coords\n",
        "#   - RFM metrics: Recency, Frequency, Monetary value per customer\n",
        "#   - delivery_performance_days & actual_delivery_days:\n",
        "#         how fast and how early/late an order was delivered\n",
        "#   - product_volume_cm3: rough size proxy for logistics complexity\n",
        "# These are domain-informed features that encode behaviour,\n",
        "# geography, and logistics performance into numeric variables.\n",
        "# =================================================================\n",
        "\n",
        "# --- 5. Haversine Distance & K-Means Clustering ---\n",
        "# Haversine Distance Function\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    # Great-circle distance between two coordinate pairs (in km)\n",
        "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return 6371 * c\n",
        "\n",
        "df['distance_km'] = haversine(\n",
        "    df['customer_lat'], df['customer_lng'],\n",
        "    df['seller_lat'], df['seller_lng']\n",
        ")\n",
        "\n",
        "# K-Means Clustering (K=10) on customer geo-coordinates\n",
        "geo_coords = df[['customer_lat', 'customer_lng']].values\n",
        "scaler = StandardScaler()\n",
        "geo_coords_scaled = scaler.fit_transform(geo_coords)\n",
        "K = 10\n",
        "# Ignore the KMeans warnings about memory management\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    kmeans = KMeans(n_clusters=K, random_state=42, n_init='auto', max_iter=300, verbose=0)\n",
        "    df['customer_geo_cluster'] = kmeans.fit_predict(geo_coords_scaled)\n",
        "df['customer_geo_cluster'] = df['customer_geo_cluster'].astype(object)\n",
        "\n",
        "# --- 6. Create RFM, Delivery, and Volume Features (as in your original code) ---\n",
        "# A. RFM Features: Recency, Frequency, Monetary at customer level\n",
        "snapshot_date = df['order_purchase_timestamp'].max() + pd.Timedelta(days=1)\n",
        "rfm_df = df.groupby('customer_unique_id').agg(\n",
        "    Recency=('order_purchase_timestamp', lambda x: (snapshot_date - x.max()).days),\n",
        "    Frequency=('order_id', 'nunique'),\n",
        "    Monetary=('payment_value', 'sum')\n",
        ").reset_index()\n",
        "df = df.merge(rfm_df, on='customer_unique_id', how='left')\n",
        "\n",
        "# B. Delivery Metrics: actual vs estimated timing\n",
        "df['delivery_time_delta'] = df['order_delivered_customer_date'] - df['order_purchase_timestamp']\n",
        "df['actual_delivery_days'] = df['delivery_time_delta'].dt.days.fillna(df['delivery_time_delta'].dt.days.median())\n",
        "df['delivery_performance_days'] = (\n",
        "    df['order_estimated_delivery_date'] - df['order_delivered_customer_date']\n",
        ").dt.days.fillna(0)\n",
        "\n",
        "# C. Product Volume (Uses aggregated product physical metrics)\n",
        "df['product_volume_cm3'] = (df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm'])\n",
        "volume_median = df['product_volume_cm3'].median()\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    df['product_volume_cm3'].fillna(volume_median, inplace=True)\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# 6. ONE-HOT ENCODING AND PRODUCT CATEGORY AGGREGATION\n",
        "# -----------------------------------------------------------------\n",
        "# main_product_category is high-cardinality and very granular.\n",
        "# Here we:\n",
        "#   - one-hot encode each detailed category,\n",
        "#   - group them into broader thematic buckets (e.g. \"Home_Kitchen_Comfort\"),\n",
        "#   - drop the original detailed dummies.\n",
        "# This reduces dimensionality while preserving useful category signal.\n",
        "# =================================================================\n",
        "\n",
        "# Note: Since we are using the aggregated 'main_product_category', we don't need the full loop.\n",
        "# We will use the same product category map for aggregation as before.\n",
        "product_prefix = 'main_product_category_'\n",
        "df_encoded = pd.get_dummies(df, columns=['main_product_category'], prefix='main_product_category', drop_first=False)\n",
        "\n",
        "columns_to_drop_after_agg = []\n",
        "category_map = {\n",
        "    'Home_Kitchen_Comfort': ['air_conditioning', 'home_appliances', 'home_appliances_2', 'home_comfort_2', 'home_confort', 'housewares', 'kitchen_dining_laundry_garden_furniture', 'la_cuisine', 'small_appliances', 'small_appliances_home_oven_and_coffee'],\n",
        "    'Fashion_Apparel': ['fashio_female_clothing', 'fashion_male_clothing', 'fashion_childrens_clothes', 'fashion_underwear_beach', 'fashion_shoes', 'fashion_sport', 'fashion_bags_accessories', 'luggage_accessories', 'watches_gifts'],\n",
        "    'Electronics_Technology': ['audio', 'cine_photo', 'computers', 'computers_accessories', 'consoles_games', 'electronics', 'fixed_telephony', 'music', 'musical_instruments', 'tablets_printing_image', 'telephony'],\n",
        "    'Tools_Construction': ['construction_tools_construction', 'construction_tools_lights', 'construction_tools_safety', 'costruction_tools_garden', 'costruction_tools_tools', 'garden_tools', 'home_construction'],\n",
        "    'Furniture_Decor': ['bed_bath_table', 'furniture_bedroom', 'furniture_decor', 'furniture_living_room', 'furniture_mattress_and_upholstery', 'office_furniture'],\n",
        "    'Books_Media_Toys': ['books_general_interest', 'books_imported', 'books_technical', 'cds_dvds_musicals', 'dvds_blu_ray', 'toys', 'cool_stuff'],\n",
        "    'Health_Personal_Care': ['health_beauty', 'perfumery', 'baby', 'diapers_and_hygiene'],\n",
        "    'Art_Hobbies': ['art', 'arts_and_craftmanship', 'stationery', 'party_supplies', 'christmas_supplies'],\n",
        "    'Food_Drink': ['drinks', 'food', 'food_drink'],\n",
        "    'Automotive': ['auto'],\n",
        "    'Other_Services': ['flowers', 'industry_commerce_and_business', 'market_place', 'pet_shop', 'security_and_services', 'signaling_and_security']\n",
        "}\n",
        "\n",
        "for new_category, detailed_categories in category_map.items():\n",
        "    original_cols = [product_prefix + cat for cat in detailed_categories]\n",
        "    existing_cols = [col for col in original_cols if col in df_encoded.columns]\n",
        "    if existing_cols:\n",
        "        df_encoded[new_category] = df_encoded[existing_cols].any(axis=1).astype(int)\n",
        "        columns_to_drop_after_agg.extend(existing_cols)\n",
        "\n",
        "sports_leisure_col = product_prefix + 'sports_leisure'\n",
        "if sports_leisure_col in df_encoded.columns:\n",
        "    df_encoded.rename(columns={sports_leisure_col: 'Sports_Leisure'}, inplace=True)\n",
        "\n",
        "df_encoded = df_encoded.drop(columns=columns_to_drop_after_agg, errors='ignore')\n",
        "df_encoded = df_encoded.drop(columns=[col for col in df_encoded.columns if col.startswith('main_product_category_') and col not in category_map], errors='ignore')\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# 7. FINAL CLEAN-UP AND TARGET VARIABLE CREATION\n",
        "# -----------------------------------------------------------------\n",
        "# At this stage we:\n",
        "#   - drop identifiers, raw text, and low-value intermediate columns,\n",
        "#   - keep only engineered, numeric/categorical model features,\n",
        "#   - create review_score_binary (1 = rating ≥4, 0 = rating ≤3),\n",
        "#   - save the final ML-ready table to CSV.\n",
        "# This dataset (V3) is the single source used by the XGBoost model.\n",
        "# =================================================================\n",
        "\n",
        "# --- 8. Final Drop of Irrelevant or Redundant Columns ---\n",
        "columns_to_drop_final = [\n",
        "    'order_id', 'customer_id', 'review_id', 'customer_unique_id',\n",
        "    'order_status', 'review_comment_title', 'review_comment_message', 'customer_city', 'seller_city',\n",
        "    'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
        "    'order_delivered_customer_date', 'order_estimated_delivery_date', 'review_creation_date',\n",
        "    'review_answer_timestamp',\n",
        "    'shipping_limit_date', # this is now present and can be dropped\n",
        "    'product_length_cm', 'product_height_cm', 'product_width_cm', 'customer_zip_code_prefix', 'seller_zip_code_prefix',\n",
        "    'delivery_time_delta',\n",
        "    # Drop raw coordinates (replaced by distance_km and clusters)\n",
        "    'customer_lat', 'customer_lng', 'seller_lat', 'seller_lng',\n",
        "    # Drop state columns (replaced by geo clustering)\n",
        "    'customer_state', 'seller_state',\n",
        "    'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g'\n",
        "]\n",
        "\n",
        "# FINAL step: create target variable and drop the raw score\n",
        "final_ml_df = df_encoded.copy()\n",
        "final_ml_df['review_score_binary'] = (final_ml_df['review_score'] >= 4).astype(int)\n",
        "\n",
        "final_ml_df = final_ml_df.drop(columns=['review_score'] + columns_to_drop_final, errors='ignore')\n",
        "\n",
        "final_ml_df.to_csv(\"ml_ready_feature_table_V3.csv\", index=False)\n",
        "\n",
        "print(f\"\\n✅ Feature engineering complete. Final corrected dataset saved as 'ml_ready_feature_table_V3.csv'. Shape: {final_ml_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Classifier"
      ],
      "metadata": {
        "id": "Y1AHwDYzWS0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 1: DATA LOADING AND SETUP\n",
        "# -----------------------------------------------------------------\n",
        "# Goal:\n",
        "#   • Load the final feature-engineered dataset (V3) where\n",
        "#       - one row = one order\n",
        "#       - all features (RFM, delivery, geo, categories, etc.) are ready.\n",
        "#   • Convert object columns to 'category' so XGBoost can handle them.\n",
        "#   • Split into train/test sets with the same class balance.\n",
        "#   • Compute class weights (scale_pos_weight) to handle imbalance\n",
        "#     between positive and negative reviews during training.\n",
        "# =================================================================\n",
        "\n",
        "# Load the final, advanced feature dataset\n",
        "df_ml = pd.read_csv(\"ml_ready_feature_table_V3.csv\")\n",
        "print(\"dataset loaded. Shape:\", df_ml.shape)\n",
        "\n",
        "# Convert object columns to 'category' for XGBoost's categorical support\n",
        "for col in df_ml.select_dtypes(include='object').columns:\n",
        "    df_ml[col] = df_ml[col].astype('category')\n",
        "\n",
        "# Target variable: 1 = Good (≥4), 0 = Bad (<4)\n",
        "y_class = df_ml['review_score_binary']\n",
        "X = df_ml.drop(columns=['review_score_binary'])\n",
        "\n",
        "# Train-test split with stratification to preserve class proportions\n",
        "X_train, X_test, y_train_class, y_test_class = train_test_split(\n",
        "    X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "# Evaluation set for XGBoost's internal tracking (train + test)\n",
        "eval_set = [(X_train, y_train_class), (X_test, y_test_class)]\n",
        "\n",
        "# Compute class weight: how many good vs bad reviews in training\n",
        "count_class_1 = y_train_class.value_counts()[1]\n",
        "count_class_0 = y_train_class.value_counts()[0]\n",
        "scale_pos_weight = count_class_1 / count_class_0\n",
        "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 2: MODEL TRAINING (XGBoost Classifier)\n",
        "# -----------------------------------------------------------------\n",
        "# Goal:\n",
        "#   • Train a cost-sensitive XGBoost model that:\n",
        "#       - handles class imbalance via scale_pos_weight\n",
        "#       - uses logloss to optimise probability predictions\n",
        "#       - works with categorical features directly.\n",
        "#   • The model learns P(review is good) and P(review is bad) for\n",
        "#     each order, which we will later convert into class labels using\n",
        "#     a custom decision threshold.\n",
        "# =================================================================\n",
        "\n",
        "xgb_final_model = XGBClassifier(\n",
        "    n_estimators=300,         # number of trees\n",
        "    max_depth=5,             # tree depth (controls complexity)\n",
        "    learning_rate=0.1,       # step size shrinkage\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    scale_pos_weight=scale_pos_weight,  # handle imbalance\n",
        "    eval_metric=\"logloss\",   # optimise log-loss (probability quality)\n",
        "    enable_categorical=True  # allow categorical dtype in features\n",
        ")\n",
        "\n",
        "print(\"\\nTraining XGBoost Model...\")\n",
        "xgb_final_model.fit(\n",
        "    X_train,\n",
        "    y_train_class,\n",
        "    eval_set=eval_set,\n",
        "    verbose=False\n",
        ")\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 3: THRESHOLD OPTIMISATION (F1 for BAD class)\n",
        "# -----------------------------------------------------------------\n",
        "# XGBoost gives probabilities, not just hard 0/1 labels.\n",
        "# By default, you would predict:\n",
        "#    class 1 if P(class 1) >= 0.5, else class 0.\n",
        "#\n",
        "# But here:\n",
        "#   • We care specifically about detecting BAD reviews (class 0),\n",
        "#     because they are more damaging.\n",
        "#   • The data is imbalanced, so 0.5 is not necessarily the best cut-off.\n",
        "#\n",
        "# Strategy:\n",
        "#   1. Use predict_proba() to get P(class=0) for each test sample.\n",
        "#   2. Sweep through thresholds from 0.50 down to 0.01.\n",
        "#   3. For each threshold:\n",
        "#        - If P(class 0) > threshold → predict 0 (bad)\n",
        "#          else → predict 1 (good)\n",
        "#        - Compute F1-score for the BAD class only (pos_label=0).\n",
        "#   4. Pick the threshold that maximises F1 for bad reviews.\n",
        "#\n",
        "# Business meaning:\n",
        "#   We tune the decision boundary so the model is as effective as\n",
        "#   possible at catching likely negative reviewers, balancing precision\n",
        "#   and recall for that group.\n",
        "# =================================================================\n",
        "\n",
        "y_proba_test = xgb_final_model.predict_proba(X_test)\n",
        "# Column 0 = probability of class 0 (bad review)\n",
        "y_proba_minority_test = y_proba_test[:, 0]\n",
        "\n",
        "thresholds = np.linspace(0.50, 0.01, 50)\n",
        "best_f1, best_threshold = 0, 0\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Predict class 0 if P(bad) > threshold, else class 1 (good)\n",
        "    y_pred_temp = np.where(y_proba_minority_test > threshold, 0, 1)\n",
        "\n",
        "    # F1 for the BAD class (treat 0 as the \"positive\" class here)\n",
        "    f1 = f1_score(y_test_class, y_pred_temp, pos_label=0)\n",
        "\n",
        "    # Keep track of the best F1 and its threshold\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_threshold = f1, threshold\n",
        "\n",
        "print(f\"\\nOptimal threshold (MAX F1-SCORE for bad class): {best_threshold:.3f} \"\n",
        "      f\"(F1 for bad class = {best_f1:.3f})\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 4: FINAL EVAL – TEST + TRAIN (same threshold)\n",
        "# -----------------------------------------------------------------\n",
        "# Goal:\n",
        "#   • Evaluate how the chosen threshold performs on:\n",
        "#       - the TEST set (generalisation)\n",
        "#       - the TRAINING set (fit/overfit comparison)\n",
        "#   • We:\n",
        "#       - generate predictions using the optimised threshold,\n",
        "#       - print precision/recall/F1 for both classes,\n",
        "#       - show the confusion matrices,\n",
        "#       - compare macro F1 between train and test.\n",
        "#\n",
        "# If train >> test → likely overfitting.\n",
        "# If train ≈ test → model generalises reasonably well.\n",
        "# =================================================================\n",
        "\n",
        "# ---------- TEST SET ----------\n",
        "y_pred_test = np.where(y_proba_minority_test > best_threshold, 0, 1)\n",
        "\n",
        "print(\"\\n--- TEST SET PERFORMANCE (F1-OPTIMIZED) ---\")\n",
        "print(classification_report(y_test_class, y_pred_test, target_names=['0 (Bad)', '1 (Good)']))\n",
        "\n",
        "cm_test = confusion_matrix(y_test_class, y_pred_test)\n",
        "cm_test_df = pd.DataFrame(\n",
        "    cm_test,\n",
        "    index=['Actual Bad (0)', 'Actual Good (1)'],\n",
        "    columns=['Predicted Bad (0)', 'Predicted Good (1)']\n",
        ")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test Set):\")\n",
        "print(cm_test_df)\n",
        "\n",
        "macro_f1_test = f1_score(y_test_class, y_pred_test, average='macro')\n",
        "print(f\"\\nMacro F1 (Test): {macro_f1_test:.3f}\")\n",
        "\n",
        "# ---------- TRAIN SET ----------\n",
        "y_proba_train = xgb_final_model.predict_proba(X_train)\n",
        "y_proba_minority_train = y_proba_train[:, 0]\n",
        "y_pred_train = np.where(y_proba_minority_train > best_threshold, 0, 1)\n",
        "\n",
        "print(\"\\n--- TRAINING SET PERFORMANCE (F1-OPTIMIZED, SAME THRESHOLD) ---\")\n",
        "print(classification_report(y_train_class, y_pred_train, target_names=['0 (Bad)', '1 (Good)']))\n",
        "\n",
        "cm_train = confusion_matrix(y_train_class, y_pred_train)\n",
        "cm_train_df = pd.DataFrame(\n",
        "    cm_train,\n",
        "    index=['Actual Bad (0)', 'Actual Good (1)'],\n",
        "    columns=['Predicted Bad (0)', 'Predicted Good (1)']\n",
        ")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Train Set):\")\n",
        "print(cm_train_df)\n",
        "\n",
        "macro_f1_train = f1_score(y_train_class, y_pred_train, average='macro')\n",
        "print(f\"\\nMacro F1 (Train): {macro_f1_train:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC6SyP7ElALH",
        "outputId": "823c1a44-fa09-491c-a236-577e22329b31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset loaded. Shape: (95879, 28)\n",
            "Calculated scale_pos_weight: 3.74\n",
            "\n",
            "Training XGBoost Model...\n",
            "Model training complete.\n",
            "\n",
            "Optimal threshold (MAX F1-SCORE for bad class): 0.080 (F1 for bad class = 0.453)\n",
            "\n",
            "--- TEST SET PERFORMANCE (F1-OPTIMIZED) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     0 (Bad)       0.46      0.45      0.45      4042\n",
            "    1 (Good)       0.85      0.86      0.86     15134\n",
            "\n",
            "    accuracy                           0.77     19176\n",
            "   macro avg       0.65      0.65      0.65     19176\n",
            "weighted avg       0.77      0.77      0.77     19176\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "                 Predicted Bad (0)  Predicted Good (1)\n",
            "Actual Bad (0)                1816                2226\n",
            "Actual Good (1)               2166               12968\n",
            "\n",
            "Macro F1 (Test): 0.654\n",
            "\n",
            "--- TRAINING SET PERFORMANCE (F1-OPTIMIZED, SAME THRESHOLD) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     0 (Bad)       0.53      0.53      0.53     16166\n",
            "    1 (Good)       0.87      0.88      0.88     60537\n",
            "\n",
            "    accuracy                           0.80     76703\n",
            "   macro avg       0.70      0.70      0.70     76703\n",
            "weighted avg       0.80      0.80      0.80     76703\n",
            "\n",
            "\n",
            "Confusion Matrix (Train Set):\n",
            "                 Predicted Bad (0)  Predicted Good (1)\n",
            "Actual Bad (0)                8584                7582\n",
            "Actual Good (1)               7520               53017\n",
            "\n",
            "Macro F1 (Train): 0.704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearsons coefficient correlation (absolute correlation for dissatisfaction)"
      ],
      "metadata": {
        "id": "k8sfKOS0WWBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# --- 1. Load the dataset ---\n",
        "df_original = pd.read_csv(\"ml_ready_feature_table_V3.csv\")\n",
        "\n",
        "# --- 2. Define the binary satisfaction target ---\n",
        "TARGET_COLUMN = 'review_score_binary'\n",
        "\n",
        "# Remove the target score column from the feature correlation calculation.\n",
        "df_corr = df_original.drop(columns=[TARGET_COLUMN], errors='ignore')\n",
        "\n",
        "# Clean non-numeric columns for correlation calculation\n",
        "for col in df_corr.columns:\n",
        "    if df_corr[col].dtype == 'object' or df_corr[col].dtype.name == 'category':\n",
        "        df_corr = df_corr.drop(columns=[col])\n",
        "    elif df_corr[col].dtype == 'bool':\n",
        "        df_corr[col] = df_corr[col].astype(int)\n",
        "\n",
        "df_corr.fillna(0, inplace=True)\n",
        "df_corr[TARGET_COLUMN] = df_original[TARGET_COLUMN]\n",
        "\n",
        "\n",
        "# --- 3. Pearson correlations ---\n",
        "corr_matrix = df_corr.corr()\n",
        "target_corr = corr_matrix[TARGET_COLUMN].drop(TARGET_COLUMN)\n",
        "\n",
        "# --- 4. Select only negative features (dissatisfaction drivers) ---\n",
        "negative_corr = target_corr[target_corr < 0]\n",
        "top5_negative = negative_corr.sort_values(ascending=True).head(5)\n",
        "\n",
        "\n",
        "# --- 5. Apply Business-Friendly Naming and Prepare for Plotting ---\n",
        "name_map = {\n",
        "    'actual_delivery_days': 'Delivery: Total Shipping Days',\n",
        "    'total_freight': 'Logistics: Total Freight Value',\n",
        "    'seller_count': 'Seller Count',\n",
        "    'total_items': 'Total Items',\n",
        "    'Furniture_Decor': 'Product: Furniture/Decor',\n",
        "    'distance_km': 'Logistics: Customer-Seller Distance (KM)',\n",
        "    'payment_value': 'Total Payment Value',\n",
        "    'payment_installments': 'Payment Installments',\n",
        "}\n",
        "\n",
        "# Apply mapping and keep the data sorted by magnitude (strongest influence first)\n",
        "top5_plot = top5_negative.abs().sort_values(ascending=False)\n",
        "top5_plot.index = top5_plot.index.map(lambda x: name_map.get(x, x))\n",
        "\n",
        "\n",
        "# --- 6. Plot: clean, blue gradient, horizontal bar chart for readability ---\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "colors = sns.color_palette(\"Blues_d\", n_colors=len(top5_plot))\n",
        "\n",
        "# Generate horizontal bar plot (better for long labels)\n",
        "plt.barh(\n",
        "    y=top5_plot.index,\n",
        "    width=top5_plot.values,\n",
        "    color=colors\n",
        ")\n",
        "\n",
        "# Reverse the order so the largest bar is at the top (standard visualization practice)\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "plt.title(\n",
        "    \"Top 5 Drivers of Customer Dissatisfaction\",\n",
        "    fontsize=16,\n",
        "    fontweight=\"bold\",\n",
        "    pad=15\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Absolute Pearson Correlation Magnitude\", fontsize=12)\n",
        "plt.ylabel(\"Feature\", fontsize=12)\n",
        "\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "sns.despine(left=True, bottom=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"top_5_negative_pearson_V3_business_friendly.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nSaved as 'top_5_negative_pearson_V3_business_friendly.png'\")"
      ],
      "metadata": {
        "id": "nORgrkjhlE0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Load the dataset ---\n",
        "df_original = pd.read_csv(\"ml_ready_feature_table_V3.csv\")\n",
        "\n",
        "# --- 2. Define the binary satisfaction target ---\n",
        "TARGET_COLUMN = 'review_score_binary'\n",
        "\n",
        "# Remove the target score column from the feature correlation calculation.\n",
        "df_corr = df_original.drop(columns=[TARGET_COLUMN], errors='ignore')\n",
        "\n",
        "# Clean non-numeric columns for correlation calculation\n",
        "for col in df_corr.columns:\n",
        "    if df_corr[col].dtype == 'object' or df_corr[col].dtype.name == 'category':\n",
        "        # Drop categorical columns entirely for Pearson correlation\n",
        "        df_corr = df_corr.drop(columns=[col])\n",
        "    elif df_corr[col].dtype == 'bool':\n",
        "        df_corr[col] = df_corr[col].astype(int)\n",
        "\n",
        "df_corr.fillna(0, inplace=True)\n",
        "\n",
        "# Temporarily merge the target back for correlation calculation\n",
        "df_corr[TARGET_COLUMN] = df_original[TARGET_COLUMN]\n",
        "\n",
        "\n",
        "# --- 3. Pearson correlations ---\n",
        "corr_matrix = df_corr.corr()\n",
        "target_corr = corr_matrix[TARGET_COLUMN].drop(TARGET_COLUMN) # Correlation with all features\n",
        "\n",
        "# --- 4. Select the Top 10 Negative Features (Dissatisfaction Drivers) ---\n",
        "# MODIFICATION: Select ONLY negative values, and take the 10 most negative (lowest values)\n",
        "negative_corr = target_corr[target_corr < 0]\n",
        "\n",
        "# Top 10 most negative correlations (ascending order means most negative first)\n",
        "top10_negative = negative_corr.sort_values(ascending=True).head(10)\n",
        "\n",
        "\n",
        "# --- 5. Prepare for Plotting ---\n",
        "# Convert to positive magnitude for plotting (to see bar lengths easily)\n",
        "top10_plot = top10_negative.abs().sort_values(ascending=False)\n",
        "top10_plot = top10_plot.rename(\n",
        "    {name: f\"({top10_negative[name]:.3f}) {name}\" for name in top10_plot.index}\n",
        ")\n",
        "\n",
        "\n",
        "# --- 6. Plot: clean, blue gradient, no outlines, no numbers ---\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Blue gradient\n",
        "colors = sns.color_palette(\"Blues_d\", n_colors=len(top10_plot))\n",
        "\n",
        "plt.barh(\n",
        "    y=top10_plot.index,\n",
        "    width=top10_plot.values,\n",
        "    color=colors\n",
        ")\n",
        "\n",
        "plt.title(\n",
        "    \"Top 10 Drivers of Dissatisfaction (Most Negative Pearson Correlation)\",\n",
        "    fontsize=16,\n",
        "    fontweight=\"bold\",\n",
        "    pad=15\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Correlation with dissatisfaction\", fontsize=12)\n",
        "plt.ylabel(\"Feature (with Correlation Value)\", fontsize=12)\n",
        "\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "sns.despine(left=True, bottom=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"top_10_negative_pearson_V3.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nSaved as 'top_10_negative_pearson_V3.png'\")"
      ],
      "metadata": {
        "id": "YrPw1bugdPbA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}