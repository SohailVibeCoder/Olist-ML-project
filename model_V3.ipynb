{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzD+e07djFWmg6PD8M27gX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohailVibeCoder/Olist-ML-project/blob/main/model_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUZE7LqekR78",
        "outputId": "633d0522-c60f-4aa0-aa59-0aff61e4b2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial raw DataFrame row count (No Duplication): 99992\n",
            "\n",
            "✅ Feature engineering complete. Final corrected dataset saved as 'ml_ready_feature_table_V3.csv'. Shape: (95879, 28)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. SETUP and INITIAL DATA LOADING ---\n",
        "# NOTE: Assuming drive is already mounted and directory is set as per user's earlier input.\n",
        "\n",
        "# Load all required original datasets (using variables defined by the user previously)\n",
        "orders_df = pd.read_csv(\"olist_orders_dataset.csv\")\n",
        "order_reviews_df = pd.read_csv(\"olist_order_reviews_dataset.csv\")\n",
        "order_payments_df = pd.read_csv(\"olist_order_payments_dataset.csv\")\n",
        "customers_df = pd.read_csv(\"olist_customers_dataset.csv\")\n",
        "order_items_df = pd.read_csv(\"olist_order_items_dataset.csv\")\n",
        "products_df = pd.read_csv(\"olist_products_dataset.csv\")\n",
        "sellers_df = pd.read_csv(\"olist_sellers_dataset.csv\")\n",
        "translation_df = pd.read_csv(\"product_category_name_translation.csv\")\n",
        "geolocation_df = pd.read_csv(\"olist_geolocation_dataset.csv\")\n",
        "\n",
        "# =================================================================\n",
        "# FIX: ORDER-LEVEL AGGREGATION TO PREVENT DUPLICATION\n",
        "# =================================================================\n",
        "\n",
        "# 1. Prepare Item Data by joining with Product, Translation, AND SELLERS tables\n",
        "items_enriched = order_items_df.merge(products_df, on='product_id', how='left')\n",
        "items_enriched = items_enriched.merge(translation_df, on='product_category_name', how='left')\n",
        "# FIX: Merge sellers_df to get seller_zip_code_prefix for aggregation\n",
        "items_enriched = items_enriched.merge(sellers_df, on='seller_id', how='left')\n",
        "\n",
        "\n",
        "# CONVERSION FIX: Ensure shipping_limit_date is datetime before aggregation\n",
        "items_enriched['shipping_limit_date'] = pd.to_datetime(items_enriched['shipping_limit_date'], errors='coerce')\n",
        "\n",
        "\n",
        "# 2. Aggregate the item details back to a single row per Order ID\n",
        "items_agg = items_enriched.groupby('order_id').agg(\n",
        "    total_items=('order_item_id', 'count'),\n",
        "    total_freight=('freight_value', 'sum'),\n",
        "    total_product_price=('price', 'sum'),\n",
        "    seller_count=('seller_id', 'nunique'),\n",
        "    main_product_category=('product_category_name_english', lambda x: x.mode()[0] if not x.mode().empty else 'unknown'),\n",
        "    # Use median for product physical features\n",
        "    product_name_lenght=('product_name_lenght', 'median'),\n",
        "    product_description_lenght=('product_description_lenght', 'median'),\n",
        "    product_photos_qty=('product_photos_qty', 'median'),\n",
        "    product_weight_g=('product_weight_g', 'median'),\n",
        "    product_length_cm=('product_length_cm', 'median'),\n",
        "    product_height_cm=('product_height_cm', 'median'),\n",
        "    product_width_cm=('product_width_cm', 'median'),\n",
        "    # Add shipping_limit_date\n",
        "    shipping_limit_date=('shipping_limit_date', 'max'),\n",
        "    # This column now exists due to the merge above:\n",
        "    seller_zip_code_prefix=('seller_zip_code_prefix', lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n",
        ").reset_index()\n",
        "\n",
        "\n",
        "pay_agg = order_payments_df.groupby('order_id').agg(\n",
        "    payment_sequential=('payment_sequential', 'count'), # Count of payment types used\n",
        "    payment_installments=('payment_installments', 'max'), # Max installments used\n",
        "    payment_value=('payment_value', 'sum') # Total payment value\n",
        ").reset_index()\n",
        "\n",
        "\n",
        "# --- 4. PERFORM NON-DUPLICATING MERGE and CLEANING ---\n",
        "\n",
        "full_df = (\n",
        "    orders_df\n",
        "    .merge(order_reviews_df, on='order_id', how='left')\n",
        "    .merge(pay_agg, on='order_id', how='left') # <-- MERGE AGGREGATED PAYMENTS HERE\n",
        "    .merge(customers_df, on='customer_id', how='left')\n",
        "    .merge(items_agg, on='order_id', how='left')\n",
        ")\n",
        "\n",
        "print(f\"Initial raw DataFrame row count (No Duplication): {len(full_df)}\")\n",
        "\n",
        "# Filter Delivered, Convert Dates, Drop Core Nulls\n",
        "full_df = full_df[full_df['order_status'] == 'delivered'].copy()\n",
        "date_cols = [\n",
        "    'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
        "    'order_delivered_customer_date', 'order_estimated_delivery_date', 'review_creation_date',\n",
        "    'review_answer_timestamp' # Note: 'shipping_limit_date' is already a datetime from the aggregation fix above\n",
        "]\n",
        "for col in date_cols:\n",
        "    full_df[col] = pd.to_datetime(full_df[col], errors='coerce')\n",
        "\n",
        "# NOTE: We now check for aggregated features instead of item-level ones\n",
        "full_df.dropna(subset=['total_product_price', 'total_freight', 'review_score', 'main_product_category'], inplace=True)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    full_df['review_comment_title'].fillna('', inplace=True)\n",
        "    full_df['review_comment_message'].fillna('', inplace=True)\n",
        "\n",
        "\n",
        "# --- 4. GEOLOCATION AGGREGATION AND MERGE ---\n",
        "# Get median lat/lng for each zip code prefix\n",
        "geo_median = geolocation_df.groupby('geolocation_zip_code_prefix').agg(\n",
        "    geolocation_lat=('geolocation_lat', 'median'),\n",
        "    geolocation_lng=('geolocation_lng', 'median')\n",
        ").reset_index()\n",
        "\n",
        "df = full_df.copy()\n",
        "\n",
        "# Merge customer lat/lng\n",
        "geo_customer = geo_median.rename(columns={'geolocation_zip_code_prefix': 'customer_zip_code_prefix', 'geolocation_lat': 'customer_lat', 'geolocation_lng': 'customer_lng'})\n",
        "df = df.merge(geo_customer, on='customer_zip_code_prefix', how='left')\n",
        "\n",
        "# Merge seller lat/lng\n",
        "geo_seller = geo_median.rename(columns={'geolocation_zip_code_prefix': 'seller_zip_code_prefix', 'geolocation_lat': 'seller_lat', 'geolocation_lng': 'seller_lng'})\n",
        "df = df.merge(geo_seller, on='seller_zip_code_prefix', how='left')\n",
        "\n",
        "# Drop rows missing necessary geo-coordinates (necessary for downstream features)\n",
        "df.dropna(subset=['customer_lat', 'customer_lng', 'seller_lat', 'seller_lng'], inplace=True)\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# ADVANCED FEATURE ENGINEERING (New Geo + Old RFM/Product Logic)\n",
        "# =================================================================\n",
        "\n",
        "# --- 5. Haversine Distance & K-Means Clustering ---\n",
        "# Haversine Distance Function\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    # This logic is correct for distance calculation\n",
        "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return 6371 * c\n",
        "\n",
        "df['distance_km'] = haversine(\n",
        "    df['customer_lat'], df['customer_lng'],\n",
        "    df['seller_lat'], df['seller_lng']\n",
        ")\n",
        "\n",
        "# K-Means Clustering (K=10)\n",
        "geo_coords = df[['customer_lat', 'customer_lng']].values\n",
        "scaler = StandardScaler()\n",
        "geo_coords_scaled = scaler.fit_transform(geo_coords)\n",
        "K = 10\n",
        "# Ignore the KMeans warnings about memory management\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    kmeans = KMeans(n_clusters=K, random_state=42, n_init='auto', max_iter=300, verbose=0)\n",
        "    df['customer_geo_cluster'] = kmeans.fit_predict(geo_coords_scaled)\n",
        "df['customer_geo_cluster'] = df['customer_geo_cluster'].astype(object)\n",
        "\n",
        "# --- 6. Create RFM, Delivery, and Volume Features (as in your original code) ---\n",
        "# A. RFM Features\n",
        "snapshot_date = df['order_purchase_timestamp'].max() + pd.Timedelta(days=1)\n",
        "rfm_df = df.groupby('customer_unique_id').agg(\n",
        "    Recency=('order_purchase_timestamp', lambda x: (snapshot_date - x.max()).days),\n",
        "    Frequency=('order_id', 'nunique'),\n",
        "    Monetary=('payment_value', 'sum')\n",
        ").reset_index()\n",
        "df = df.merge(rfm_df, on='customer_unique_id', how='left')\n",
        "\n",
        "# B. Delivery Metrics\n",
        "df['delivery_time_delta'] = df['order_delivered_customer_date'] - df['order_purchase_timestamp']\n",
        "df['actual_delivery_days'] = df['delivery_time_delta'].dt.days.fillna(df['delivery_time_delta'].dt.days.median())\n",
        "df['delivery_performance_days'] = (\n",
        "    df['order_estimated_delivery_date'] - df['order_delivered_customer_date']\n",
        ").dt.days.fillna(0)\n",
        "\n",
        "# C. Product Volume (Uses aggregated product physical metrics)\n",
        "df['product_volume_cm3'] = (df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm'])\n",
        "volume_median = df['product_volume_cm3'].median()\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    df['product_volume_cm3'].fillna(volume_median, inplace=True)\n",
        "\n",
        "# --- 7. One-Hot Encoding and Product Category Aggregation (Dimensionality Reduction) ---\n",
        "# Note: Since we are using the aggregated 'main_product_category', we don't need the full loop.\n",
        "# We will use the same product category map for aggregation as before.\n",
        "product_prefix = 'main_product_category_'\n",
        "df_encoded = pd.get_dummies(df, columns=['main_product_category'], prefix='main_product_category', drop_first=False)\n",
        "\n",
        "columns_to_drop_after_agg = []\n",
        "category_map = {\n",
        "    'Home_Kitchen_Comfort': ['air_conditioning', 'home_appliances', 'home_appliances_2', 'home_comfort_2', 'home_confort', 'housewares', 'kitchen_dining_laundry_garden_furniture', 'la_cuisine', 'small_appliances', 'small_appliances_home_oven_and_coffee'],\n",
        "    'Fashion_Apparel': ['fashio_female_clothing', 'fashion_male_clothing', 'fashion_childrens_clothes', 'fashion_underwear_beach', 'fashion_shoes', 'fashion_sport', 'fashion_bags_accessories', 'luggage_accessories', 'watches_gifts'],\n",
        "    'Electronics_Technology': ['audio', 'cine_photo', 'computers', 'computers_accessories', 'consoles_games', 'electronics', 'fixed_telephony', 'music', 'musical_instruments', 'tablets_printing_image', 'telephony'],\n",
        "    'Tools_Construction': ['construction_tools_construction', 'construction_tools_lights', 'construction_tools_safety', 'costruction_tools_garden', 'costruction_tools_tools', 'garden_tools', 'home_construction'],\n",
        "    'Furniture_Decor': ['bed_bath_table', 'furniture_bedroom', 'furniture_decor', 'furniture_living_room', 'furniture_mattress_and_upholstery', 'office_furniture'],\n",
        "    'Books_Media_Toys': ['books_general_interest', 'books_imported', 'books_technical', 'cds_dvds_musicals', 'dvds_blu_ray', 'toys', 'cool_stuff'],\n",
        "    'Health_Personal_Care': ['health_beauty', 'perfumery', 'baby', 'diapers_and_hygiene'],\n",
        "    'Art_Hobbies': ['art', 'arts_and_craftmanship', 'stationery', 'party_supplies', 'christmas_supplies'],\n",
        "    'Food_Drink': ['drinks', 'food', 'food_drink'],\n",
        "    'Automotive': ['auto'],\n",
        "    'Other_Services': ['flowers', 'industry_commerce_and_business', 'market_place', 'pet_shop', 'security_and_services', 'signaling_and_security']\n",
        "}\n",
        "\n",
        "for new_category, detailed_categories in category_map.items():\n",
        "    original_cols = [product_prefix + cat for cat in detailed_categories]\n",
        "    existing_cols = [col for col in original_cols if col in df_encoded.columns]\n",
        "    if existing_cols:\n",
        "        df_encoded[new_category] = df_encoded[existing_cols].any(axis=1).astype(int)\n",
        "        columns_to_drop_after_agg.extend(existing_cols)\n",
        "\n",
        "sports_leisure_col = product_prefix + 'sports_leisure'\n",
        "if sports_leisure_col in df_encoded.columns:\n",
        "    df_encoded.rename(columns={sports_leisure_col: 'Sports_Leisure'}, inplace=True)\n",
        "\n",
        "df_encoded = df_encoded.drop(columns=columns_to_drop_after_agg, errors='ignore')\n",
        "df_encoded = df_encoded.drop(columns=[col for col in df_encoded.columns if col.startswith('main_product_category_') and col not in category_map], errors='ignore')\n",
        "\n",
        "\n",
        "# --- 8. Final Drop of Irrelevant or Redundant Columns ---\n",
        "columns_to_drop_final = [\n",
        "    'order_id', 'customer_id', 'review_id', 'customer_unique_id',\n",
        "    'order_status', 'review_comment_title', 'review_comment_message', 'customer_city', 'seller_city',\n",
        "    'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
        "    'order_delivered_customer_date', 'order_estimated_delivery_date', 'review_creation_date',\n",
        "    'review_answer_timestamp',\n",
        "    'shipping_limit_date', # <-- THIS IS NOW PRESENT AND CAN BE DROPPED\n",
        "    'product_length_cm', 'product_height_cm', 'product_width_cm', 'customer_zip_code_prefix', 'seller_zip_code_prefix',\n",
        "    'delivery_time_delta',\n",
        "    # DROPPING RAW GEO COORDINATES (now replaced by distance_km and clusters)\n",
        "    'customer_lat', 'customer_lng', 'seller_lat', 'seller_lng',\n",
        "    # Dropping the old state columns which were intended to be dropped in the original code\n",
        "    'customer_state', 'seller_state',\n",
        "    'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g'\n",
        "]\n",
        "\n",
        "# FINAL step: create target variable and drop the raw score\n",
        "final_ml_df = df_encoded.copy()\n",
        "final_ml_df['review_score_binary'] = (final_ml_df['review_score'] >= 4).astype(int)\n",
        "\n",
        "final_ml_df = final_ml_df.drop(columns=['review_score'] + columns_to_drop_final, errors='ignore')\n",
        "\n",
        "final_ml_df.to_csv(\"ml_ready_feature_table_V3.csv\", index=False)\n",
        "\n",
        "print(f\"\\n✅ Feature engineering complete. Final corrected dataset saved as 'ml_ready_feature_table_V3.csv'. Shape: {final_ml_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 1: DATA LOADING AND SETUP\n",
        "# =================================================================\n",
        "# Load the final, advanced feature dataset\n",
        "df_ml = pd.read_csv(\"ml_ready_feature_table_V3.csv\")\n",
        "print(\"dataset loaded. Shape:\", df_ml.shape)\n",
        "\n",
        "# --- FIX: Convert object columns to the 'category' dtype for XGBoost ---\n",
        "for col in df_ml.select_dtypes(include='object').columns:\n",
        "    df_ml[col] = df_ml[col].astype('category')\n",
        "# --- END FIX ---\n",
        "\n",
        "# Target variable: 1 = Good (≥4), 0 = Bad (<4)\n",
        "y_class = df_ml['review_score_binary']\n",
        "X = df_ml.drop(columns=['review_score_binary'])\n",
        "\n",
        "# --- 2. Train-Test Split and Class Weighting ---\n",
        "X_train, X_test, y_train_class, y_test_class = train_test_split(\n",
        "    X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "eval_set = [(X_train, y_train_class), (X_test, y_test_class)]\n",
        "\n",
        "count_class_1 = y_train_class.value_counts()[1]\n",
        "count_class_0 = y_train_class.value_counts()[0]\n",
        "scale_pos_weight = count_class_1 / count_class_0\n",
        "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 2: MODEL TRAINING\n",
        "# =================================================================\n",
        "# --- 3. Train XGBoost Classifier ---\n",
        "xgb_final_model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    eval_metric=\"logloss\",\n",
        "    enable_categorical=True\n",
        ")\n",
        "print(\"\\nTraining XGBoost Model...\")\n",
        "xgb_final_model.fit(\n",
        "    X_train,\n",
        "    y_train_class,\n",
        "    eval_set=eval_set,\n",
        "    verbose=False\n",
        ")\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# =================================================================\n",
        "# SECTION 3: EVALUATION AND FEATURE IMPORTANCE\n",
        "# =================================================================\n",
        "# --- 5. Threshold Optimization (REVERTING TO F1-SCORE for Balance) ---\n",
        "y_proba = xgb_final_model.predict_proba(X_test)\n",
        "y_proba_minority = y_proba[:, 0]  # Probability of being class 0 (bad review)\n",
        "\n",
        "# Find the optimal threshold for the best F1 score on the Bad Review class\n",
        "thresholds = np.linspace(0.50, 0.01, 50)\n",
        "best_f1, best_threshold = 0, 0\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_temp = np.where(y_proba_minority > threshold, 0, 1)\n",
        "\n",
        "    # ***MODIFICATION: Reverting back to F1-Score calculation***\n",
        "    f1 = f1_score(y_test_class, y_pred_temp, pos_label=0)\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_threshold = f1, threshold\n",
        "\n",
        "# NOTE: The variable names are updated to reflect the new metric\n",
        "print(f\"\\nOptimal threshold (MAX F1-SCORE): {best_threshold:.3f} (F1 for bad class = {best_f1:.3f})\")\n",
        "\n",
        "\n",
        "# --- 6. Final Evaluation (TEST Set + Confusion Matrix) ---\n",
        "y_pred_final = np.where(y_proba_minority > best_threshold, 0, 1)\n",
        "\n",
        "print(\"\\n--- MODEL TEST SET PERFORMANCE (F1-OPTIMIZED) ---\")\n",
        "print(classification_report(y_test_class, y_pred_final, target_names=['0 (Bad)', '1 (Good)']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_class, y_pred_final)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test Set):\")\n",
        "print(cm)\n",
        "\n",
        "# print version\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=['Actual Bad (0)', 'Actual Good (1)'],\n",
        "    columns=['Predicted Bad (0)', 'Predicted Good (1)']\n",
        ")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Formatted):\")\n",
        "print(cm_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC6SyP7ElALH",
        "outputId": "93d8780d-b631-41b8-d514-086f23393c6e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset loaded. Shape: (95879, 28)\n",
            "Calculated scale_pos_weight: 3.74\n",
            "\n",
            "Training XGBoost Model...\n",
            "Model training complete.\n",
            "\n",
            "Optimal threshold (MAX F1-SCORE): 0.080 (F1 for bad class = 0.453)\n",
            "\n",
            "--- MODEL TEST SET PERFORMANCE (F1-OPTIMIZED) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     0 (Bad)       0.46      0.45      0.45      4042\n",
            "    1 (Good)       0.85      0.86      0.86     15134\n",
            "\n",
            "    accuracy                           0.77     19176\n",
            "   macro avg       0.65      0.65      0.65     19176\n",
            "weighted avg       0.77      0.77      0.77     19176\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[ 1816  2226]\n",
            " [ 2166 12968]]\n",
            "\n",
            "Confusion Matrix (Formatted):\n",
            "                 Predicted Bad (0)  Predicted Good (1)\n",
            "Actual Bad (0)                1816                2226\n",
            "Actual Good (1)               2166               12968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Load the dataset ---\n",
        "df_original = pd.read_csv(\"ml_ready_feature_table_V3.csv\")\n",
        "\n",
        "# --- 2. Create binary satisfaction target ---\n",
        "df_original['review_score_binary'] = (df_original['review_score'] >= 4).astype(int)\n",
        "TARGET_COLUMN = 'review_score_binary'\n",
        "\n",
        "# Remove raw score\n",
        "df_corr = df_original.drop(columns=['review_score'], errors='ignore')\n",
        "\n",
        "# Clean boolean columns\n",
        "for col in df_corr.columns:\n",
        "    if df_corr[col].dtype == 'bool':\n",
        "        df_corr[col] = df_corr[col].astype(int)\n",
        "\n",
        "df_corr.fillna(0, inplace=True)\n",
        "\n",
        "# --- 3. Pearson correlations ---\n",
        "corr_matrix = df_corr.corr()\n",
        "target_corr = corr_matrix[TARGET_COLUMN].drop(TARGET_COLUMN)\n",
        "\n",
        "# --- 4. Select only negative features (dissatisfaction drivers) ---\n",
        "negative_corr = target_corr[target_corr < 0]\n",
        "\n",
        "# Top 5 most negative correlations\n",
        "top5_negative = negative_corr.sort_values().head(5)\n",
        "\n",
        "# --- 5. Business-friendly naming ---\n",
        "#name_map = {\n",
        "   # 'delivery_performance_days': 'Delivery: Days Ahead/Behind Estimate',\n",
        "  #  'actual_delivery_days': 'Delivery: Total Shipping Days',\n",
        "   # 'distance_km': 'Logistics: Customer-Seller Distance (KM)',\n",
        "   # 'Monetary': 'Customer Value (Monetary)',\n",
        "   # 'payment_value': 'Total Payment Value',\n",
        "   # 'Furniture_Decor': 'Product: Furniture/Decor',\n",
        "   # 'customer_geo_cluster_1': 'Geo Hub 1',\n",
        "   # 'payment_installments': 'Payment Installments',\n",
        "   # 'freight_value': 'Item Freight Value',\n",
        "   # 'Books_Media_Toys': 'Product: Books/Media/Toys'\n",
        "#}\n",
        "\n",
        "#top5_negative.index = top5_negative.index.map(lambda x: name_map.get(x, x))\n",
        "\n",
        "# Convert to positive for plotting (but it's still negative behind the scenes)\n",
        "top5_plot = top5_negative.abs().sort_values(ascending=False)\n",
        "\n",
        "# --- 6. Plot: clean, blue gradient, no outlines, no numbers ---\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Blue gradient\n",
        "colors = sns.color_palette(\"Blues\", n_colors=len(top5_plot))\n",
        "\n",
        "plt.bar(\n",
        "    x=np.arange(len(top5_plot)),\n",
        "    height=top5_plot.values,\n",
        "    color=colors\n",
        ")\n",
        "\n",
        "plt.xticks(\n",
        "    ticks=np.arange(len(top5_plot)),\n",
        "    labels=top5_plot.index,\n",
        "    rotation=45,\n",
        "    ha='right',\n",
        "    fontsize=10\n",
        ")\n",
        "\n",
        "plt.title(\n",
        "    \"Top 5 Drivers of Customer Dissatisfaction\",\n",
        "    fontsize=16,\n",
        "    fontweight=\"bold\",\n",
        "    pad=15\n",
        ")\n",
        "\n",
        "plt.ylabel(\"Pearsons Correlation Coefficient\", fontsize=12)\n",
        "plt.xlabel(\"Feature\", fontsize=12)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
        "sns.despine(left=True, bottom=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"top_5_negative_pearson_.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nSaved as 'top_5_negative_pearson_V3.png'\")\n"
      ],
      "metadata": {
        "id": "nORgrkjhlE0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}